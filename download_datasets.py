"""
Download and prepare datasets for training.
- Downloads FineWeb-Edu for pretraining (configurable size)
- Downloads MetaMathQA for basic math (algebra, arithmetic, geometry)
- Downloads OpenMathInstruct-2 for math reasoning (GSM8K+MATH augmented, 14M problems)
- Downloads MathInstruct for diverse math (13 sources incl. calculus, TheoremQA)
- Downloads SFT datasets (sft-ultra)
- Downloads NuminaMath-CoT for chain-of-thought reasoning

All SFT data is normalized to [INST]...[/INST] format for consistency.
Pretraining data includes <EOS> delimiters between documents.

Usage:
    python download_datasets.py
"""

import os
import json
from pathlib import Path
from tqdm import tqdm
from datasets import load_dataset

# Special tokens for consistency
# MUST match tokens defined in tokenizer/src/lib.rs SPECIAL_TOKENS
END_OF_TEXT = "<EOS>"          # Document delimiter (maps to <EOS> special token)
INST_START = "[INST]"          # Instruction start (special token)
INST_END = "[/INST]"           # Instruction end (special token)
SYS_START = "<|system|>"       # System message marker (special token)
SYS_END = "<|system|>"         # System message end (reuse same token)
EOS_TOKEN = "<EOS>"            # End of sequence (special token)

# Output directory
DATASET_DIR = Path(__file__).parent / "dataset"
DATASET_DIR.mkdir(parents=True, exist_ok=True)

# Dataset configurations - size in bytes
# Adjust size_bytes for your needs (e.g., 25GB for H200 training)

# =============================================================================
# SET THIS TO True FOR H200 TRAINING (larger datasets)
# =============================================================================
H200_MODE = True  # Set to True for 25GB pretraining data

DATASETS = {
    "fineweb": {
        "path": "HuggingFaceFW/fineweb-edu",
        "name": "sample-10BT",
        "split": "train",
        # For H200: 25GB, otherwise 1GB
        "size_bytes": (25 * 1024 * 1024 * 1024) if H200_MODE else (1 * 1024 * 1024 * 1024),
        "format": "pretrain",
        "text_key": "text"
    },
    "math-basic": {
        "path": "meta-math/MetaMathQA",
        "name": None,
        "split": "train",
        # MetaMathQA: ~395k augmented problems (algebra, arithmetic, geometry, word problems)
        # Total dataset is ~396MB — use all of it
        "size_bytes": (400 * 1024 * 1024) if H200_MODE else (100 * 1024 * 1024),
        "format": "qa",
        "question_key": "query",
        "answer_key": "response"
    },
    "math-openmath2": {
        "path": "nvidia/OpenMathInstruct-2",
        "name": None,
        # train_2M = 2M curated subset, train_5M = 5M, train = full 14M
        "split": "train_5M" if H200_MODE else "train_1M",
        # OpenMathInstruct-2: up to 14M problems generated by Llama-3.1-405B
        # GSM8K + MATH augmented with CoT solutions
        # ~12.6GB total, 5M split ~ 4.5GB, 1M split ~ 900MB
        "size_bytes": (5 * 1024 * 1024 * 1024) if H200_MODE else (500 * 1024 * 1024),
        "format": "openmath",
        "question_key": "problem",
        "answer_key": "generated_solution"
    },
    "math-instruct": {
        "path": "TIGER-Lab/MathInstruct",
        "name": None,
        "split": "train",
        # MathInstruct: 262k problems from 13 sources
        # Includes: GSM8K, MATH, AQuA-RAT, TheoremQA, CAMEL-Math, NumGLUE, MathQA
        # Mix of CoT and PoT (program-of-thought) rationales
        # Total dataset ~212MB — use all of it
        "size_bytes": (250 * 1024 * 1024) if H200_MODE else (100 * 1024 * 1024),
        "format": "qa",
        "question_key": "instruction",
        "answer_key": "output"
    },
    "sft-ultra": {
        "path": "HuggingFaceH4/ultrachat_200k",
        "name": "default",
        "split": "train_sft",
        # For H200: 1GB, otherwise 50MB
        "size_bytes": (1 * 1024 * 1024 * 1024) if H200_MODE else (50 * 1024 * 1024),
        "format": "messages",
        "messages_key": "messages"
    },
    "math-sft-plus": {
        "path": "AI-MO/NuminaMath-CoT",
        "name": None,
        "split": "train",
        # For H200: 500MB, otherwise 50MB
        "size_bytes": (500 * 1024 * 1024) if H200_MODE else (50 * 1024 * 1024),
        "format": "cot",  # Chain-of-thought format
        "question_key": "problem",
        "answer_key": "solution"
    }
}


def format_item(item: dict, config: dict) -> str:
    """
    Format a dataset item into normalized text.
    
    - pretrain: raw text with <EOS> delimiter
    - qa: [INST] question [/INST] answer
    - messages: multi-turn [INST]...[/INST] format
    - cot: [INST] with "step by step" prefix
    """
    format_type = config.get("format", "pretrain")
    
    if format_type == "pretrain":
        # Pretraining data: raw text with document delimiter
        text_key = config.get("text_key", "text")
        text = item.get(text_key, "")
        if text:
            return text.strip() + " " + END_OF_TEXT
        return ""
    
    elif format_type == "qa":
        # Question-Answer format
        q_key = config.get("question_key", "question")
        a_key = config.get("answer_key", "answer")
        question = item.get(q_key, "").strip()
        answer = item.get(a_key, "").strip()
        
        if not question or not answer:
            return ""
        
        return f"{INST_START} {question} {INST_END} {answer} {EOS_TOKEN}"
    
    elif format_type == "cot":
        # Chain-of-thought format with explicit instruction
        q_key = config.get("question_key", "problem")
        a_key = config.get("answer_key", "solution")
        question = item.get(q_key, "").strip()
        answer = item.get(a_key, "").strip()
        
        if not question or not answer:
            return ""
        
        instruction = f"Solve this problem step by step:\n\n{question}"
        return f"{INST_START} {instruction} {INST_END} {answer} {EOS_TOKEN}"
    
    elif format_type == "messages":
        # Multi-turn conversation format
        messages_key = config.get("messages_key", "messages")
        messages = item.get(messages_key, [])
        
        if not messages:
            return ""
        
        formatted_parts = []
        current_user = None
        system_msg = None
        
        for msg in messages:
            role = msg.get("role", "")
            content = msg.get("content", "").strip()
            
            if role == "system":
                system_msg = content
            elif role == "user":
                current_user = content
            elif role == "assistant" and current_user:
                # Include system message in first turn only
                if system_msg and not formatted_parts:
                    user_content = f"{SYS_START}\n{system_msg}\n{SYS_END}\n\n{current_user}"
                else:
                    user_content = current_user
                
                formatted_parts.append(
                    f"{INST_START} {user_content} {INST_END} {content}"
                )
                current_user = None
        
        if formatted_parts:
            return " ".join(formatted_parts) + " " + EOS_TOKEN
        return ""
    
    elif format_type == "camel":
        # CAMEL AI conversational math format
        # message_1 = user question, message_2 = assistant answer
        m1_key = config.get("message1_key", "message_1")
        m2_key = config.get("message2_key", "message_2")
        question = item.get(m1_key, "").strip()
        answer = item.get(m2_key, "").strip()
        
        if not question or not answer:
            return ""
        
        return f"{INST_START} {question} {INST_END} {answer} {EOS_TOKEN}"
    
    elif format_type == "openmath":
        # OpenMathInstruct-2 format: problem + generated_solution
        q_key = config.get("question_key", "problem")
        a_key = config.get("answer_key", "generated_solution")
        question = item.get(q_key, "").strip()
        answer = item.get(a_key, "").strip()
        
        if not question or not answer:
            return ""
        
        return f"{INST_START} {question} {INST_END} {answer} {EOS_TOKEN}"
    
    # Fallback
    return str(item)


def download_dataset(name: str, config: dict):
    """Download a single dataset."""
    output_file = DATASET_DIR / f"{name}.jsonl"
    
    # Check if already exists with sufficient size
    if output_file.exists():
        size = output_file.stat().st_size
        target_size = config["size_bytes"]
        if size >= target_size * 0.9:  # Allow 10% tolerance
            print(f"[{name}] Already downloaded ({size / (1024*1024):.1f}MB)")
            return
    
    print(f"\n{'='*60}")
    print(f"Downloading: {name}")
    print(f"Source: {config['path']}")
    target_mb = config["size_bytes"] / (1024 * 1024)
    target_gb = config["size_bytes"] / (1024 * 1024 * 1024)
    if target_gb >= 1:
        print(f"Target size: {target_gb:.1f}GB")
    else:
        print(f"Target size: {target_mb:.1f}MB")
    print(f"Format: {config.get('format', 'pretrain')}")
    print(f"{'='*60}")
    
    # Load dataset with streaming
    load_args = {
        "path": config["path"],
        "split": config["split"],
        "streaming": True
    }
    
    if config.get("name"):
        load_args["name"] = config["name"]
    
    try:
        dataset = load_dataset(**load_args)
    except Exception as e:
        print(f"[{name}] Error loading dataset: {e}")
        return
    
    # Collect and format data
    current_size = 0
    target_size = config["size_bytes"]
    samples = []
    skipped = 0
    
    pbar_unit = "GB" if target_size >= 1024*1024*1024 else "MB"
    pbar_divisor = 1024*1024*1024 if pbar_unit == "GB" else 1024*1024
    pbar = tqdm(
        total=target_size / pbar_divisor,
        desc=f"Downloading {name}",
        unit=pbar_unit,
        bar_format='{l_bar}{bar}| {n:.2f}/{total:.2f}{unit} [{elapsed}<{remaining}]'
    )
    last_pbar_val = 0
    
    for item in dataset:
        text = format_item(item, config)
        
        if not text:
            skipped += 1
            continue
            
        text_bytes = text.encode('utf-8')
        size = len(text_bytes)
        
        samples.append({"text": text})
        current_size += size
        
        # Update progress
        current_unit = current_size / pbar_divisor
        if current_unit - last_pbar_val >= 0.01:
            pbar.update(current_unit - last_pbar_val)
            last_pbar_val = current_unit
        
        if current_size >= target_size:
            break
    
    pbar.close()
    
    if skipped > 0:
        print(f"[{name}] Skipped {skipped} empty/invalid samples")
    
    # Save as JSONL
    print(f"[{name}] Saving {len(samples):,} samples to {output_file}...")
    with open(output_file, 'w', encoding='utf-8') as f:
        for sample in samples:
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')
    
    actual_size = output_file.stat().st_size
    actual_mb = actual_size / (1024 * 1024)
    actual_gb = actual_size / (1024 * 1024 * 1024)
    
    if actual_gb >= 1:
        print(f"[{name}] Done! Saved {actual_gb:.2f}GB ({len(samples):,} samples)")
    else:
        print(f"[{name}] Done! Saved {actual_mb:.2f}MB ({len(samples):,} samples)")


def main():
    print("="*60)
    print("DATASET DOWNLOAD SCRIPT")
    print("="*60)
    print(f"Output directory: {DATASET_DIR}")
    print(f"\nDatasets to download:")
    for name, config in DATASETS.items():
        size_mb = config["size_bytes"] / (1024 * 1024)
        size_gb = config["size_bytes"] / (1024 * 1024 * 1024)
        fmt = config.get("format", "pretrain")
        if size_gb >= 1:
            print(f"  - {name}: {size_gb:.1f}GB ({fmt})")
        else:
            print(f"  - {name}: {size_mb:.1f}MB ({fmt})")
    print("="*60)
    
    for name, config in DATASETS.items():
        try:
            download_dataset(name, config)
        except Exception as e:
            print(f"[{name}] Failed: {e}")
            continue
    
    print("\n" + "="*60)
    print("DOWNLOAD COMPLETE!")
    print("="*60)
    
    # Show summary
    total_size = 0
    print("\nGenerated files:")
    for file in sorted(DATASET_DIR.glob("*.jsonl")):
        size = file.stat().st_size
        total_size += size
        size_mb = size / (1024 * 1024)
        size_gb = size / (1024 * 1024 * 1024)
        if size_gb >= 1:
            print(f"  {file.name}: {size_gb:.2f}GB")
        else:
            print(f"  {file.name}: {size_mb:.2f}MB")
    
    total_gb = total_size / (1024 * 1024 * 1024)
    print(f"\nTotal: {total_gb:.2f}GB")
    print(f"Location: {DATASET_DIR}")
    print("="*60)


if __name__ == "__main__":
    main()

